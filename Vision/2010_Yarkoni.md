# Yarkoni et al (2010): Cognitive neuroscience 2.0: Building a cumulative science of human brain function

## Main takeaway: Review of approaches to synthesize and integrate data across studies and labs

* Critical experiment vs synthesis (data + modeling)
	* synthesis more data efficient
	* Allows to aggregate many different experiments - meta-analysis
	* No single experiment can account for all co-founding factors!

## Motivation

* Underpowered studies
	* Costs of experiments - small sample sizes
	* Multiple comparison problem - little power to detect anything else than very large effects - many false positives (not conservative enough corrections)
	* Illusion of selective activation
	* Bias in literature to report significant results
	* Meta-analysis
		* capture effects consistent across labs and task variants
		* Separate consistent findings from idiosyncratic ones
	* FPR - even very small significance levels not enough to control problem across entire volume of the brain

* Not many direct replications
	* Instead „conceptual“: add new manipulation/context and retain substantial previous features
	* Analysis based on vowel activations vs location of activated regions
		* Solution: Consensus methods

* Hard to identify selective functional association - Structure-function associations
	* activation of region by multiple functions - correlation vs causation
	* Need formal structure = large-scale functional-anatomical organization
	* Default vs task-positive networks

## Integration

* Integration of data: spatial framework, data format, access to data
	* standardized meta-analysis software
	* Ontology - describe conceptual structure - basis for annotation

* Need automated extraction of coordinates - set standards for publishing

* Image-based vs Foci-based analysis = Images preserve full range of effect sizes in data - power boost while min selection bias

* Move towards open science and reproducibility

## Future

1. Fully automated quantitative mapping between cognitive and neural states
2. Intelligent preprocessing and analysis pipelines
3. Integration or interoperation of neuroimaging DBs with other types of data (functional genomics)
4. Deployment massive data repositories
5. P2P data collaboration
6. Integration of ontologies and data sharing methods with analysis software